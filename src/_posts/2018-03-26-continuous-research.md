---
layout: post
title: "Continuous Research"
author: "Alex Morley"
date: 2018-03-26
---

Share your data. Share your code. Write some tests. Stop using proprietary formats/software. There are (to my mind) a lot of obvious benefits to doing these things in their own right, not least that they are pretty much essential to ensure the integrity of any research. Without data & code how do I know that you haven't made a mistake, fudged a figure, or just chosen only the very specific result that supports your hypothesis. Professional software developers don't trust themselves to write bug-free code, so I never understood why scientists get a pass on this.

But today isn't about moaning about the problems with *not sharing* but about sharing a blue-sky project made possible by everyone who *is sharing* and moving research into the 21st century.

The requirement to maintain large, distributed, and complex projects, along with the high availability of low-cost computing has driven the widespread adoption of "Continuous Delivery" in software development. The mechanics can be complex, but the principle is simple: automated tests are frequently run against the software so that even when a brand new feature is added, you can be sure that the software is still (at least close to) functional enough to be released immediately. The tooling available to do this has become pretty easy to set-up and as such the primary overhead of using a technique is writing good-enough tests, not trivial, but something that probably should be done anyway.

So what has this got to do with research. Well, let's have a look at a research flow that I beleive encompasses a least a majority of scientific research.
